{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Offensive Detection: Preprocessing and Embedding\n",
    " \n",
    "In this notebook, we perform the following steps:\n",
    " \n",
    "1. **Data Loading and Exploration**: Load the training data and inspect its columns.\n",
    "2. **Data Preprocessing**: Drop unnecessary columns and clean the tweet text by removing URLs, punctuation, stopwords, and extra spaces.\n",
    "3. **Saving Preprocessed Data**: Save the cleaned dataset as `train_prepro.csv`.\n",
    "4. **Embedding using GloVe**: Tokenize the cleaned tweet text, convert it to padded sequences, and create an embedding matrix using pretrained GloVe embeddings.\n",
    "5. **Saving the Embedding Matrix**: Save the embedding matrix for later use in model building.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data Loading and Exploration\n",
    " \n",
    "In this section, we load the CSV file that contains the training data. The dataset includes the following columns:\n",
    "- `count`\n",
    "- `hate_speech_count`\n",
    "- `offensive_language_count`\n",
    "- `neither_count`\n",
    "- `tweet`\n",
    "- `class`\n",
    " \n",
    "We will inspect the data to understand its structure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the dataset:\n",
      "Index(['count', 'hate_speech_count', 'offensive_language_count',\n",
      "       'neither_count', 'tweet', 'class'],\n",
      "      dtype='object')\n",
      "\n",
      "Training Data Sample:\n",
      "   count  hate_speech_count  offensive_language_count  neither_count  \\\n",
      "0      3                  2                         0              1   \n",
      "1      3                  0                         0              3   \n",
      "2      3                  0                         3              0   \n",
      "3      3                  0                         3              0   \n",
      "4      6                  0                         6              0   \n",
      "\n",
      "                                               tweet  class  \n",
      "0  RT @FunSizedYogi: @TheBlackVoice well how else...      0  \n",
      "1  Funny thing is....it's not just the people doi...      2  \n",
      "2  RT @winkSOSA: \"@AintShitSweet__: \"@Rakwon_OGOD...      1  \n",
      "3  @Jbrendaro30 @ZGabrail @ramsin1995 @GabeEli8 @...      1  \n",
      "4                                S/o that real bitch      1  \n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(r'C:\\Users\\hp\\Downloads\\nlp ass2\\train (2).csv')\n",
    "\n",
    "print(\"Columns in the dataset:\")\n",
    "print(train_df.columns)\n",
    "print(\"\\nTraining Data Sample:\")\n",
    "print(train_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Data Preprocessing\n",
    " \n",
    "In this section, we:\n",
    "- **Drop Unnecessary Columns**: We only need the `tweet` and `class` columns for text classification.\n",
    "- **Clean the Tweet Text**: Remove URLs, punctuation, stopwords, and extra spaces to reduce noise in the text.\n",
    " \n",
    "The cleaned text will be saved in a new column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tokenized and padded tweet data: (19826, 100)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Define a function to clean tweet text\n",
    "def clean_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove stopwords\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Join tokens back to string\n",
    "    cleaned_text = \" \".join(tokens)\n",
    "    return cleaned_text\n",
    "\n",
    "train_df = pd.read_csv(r'C:\\Users\\hp\\Downloads\\nlp ass2\\train (2).csv')\n",
    "\n",
    "train_df_clean = train_df[['tweet', 'class']].copy()\n",
    "\n",
    "train_df_clean['clean_tweet'] = train_df_clean['tweet'].apply(clean_text)\n",
    "\n",
    "max_words = 10000 \n",
    "max_len = 100     \n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(train_df_clean['clean_tweet'])  \n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(train_df_clean['clean_tweet'])\n",
    "X = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "print(\"Shape of tokenized and padded tweet data:\", X.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Saving Preprocessed Data\n",
    " \n",
    "We now save the preprocessed training data (with the cleaned tweets) as `train_prepro.csv` so that it can be reused later in the pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed training data saved as 'train_prepro.csv'\n"
     ]
    }
   ],
   "source": [
    "train_df_clean.to_csv('train_prepro.csv', index=False)\n",
    "print(\"Preprocessed training data saved as 'train_prepro.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Embedding using GloVe\n",
    " \n",
    "In this section, we:\n",
    "- **Tokenize the Cleaned Tweet Text**: Convert the text into sequences of integers.\n",
    "- **Pad the Sequences**: Ensure each sequence has a fixed length.\n",
    "- **Load Pretrained GloVe Embeddings**: Load the GloVe file (e.g., `glove.6B.100d.txt`) and create an embedding matrix that maps words in our vocabulary to their GloVe vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in GloVe: 400000\n"
     ]
    }
   ],
   "source": [
    "# Load GloVe embeddings\n",
    "embedding_index = {}\n",
    "glove_file = 'glove.6B.100d.txt'  \n",
    "\n",
    "with open(glove_file, encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_index[word] = coefs\n",
    "\n",
    "print(\"Total number of words in GloVe:\", len(embedding_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: (10000, 100)\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 100  \n",
    "word_index = tokenizer.word_index\n",
    "num_words = min(max_words, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_words:\n",
    "        continue\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print(\"Embedding matrix shape:\", embedding_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Saving the Embedding Matrix\n",
    " \n",
    "We now save the embedding matrix to a file so that it can be loaded later during model training without needing to reprocess the GloVe file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix saved as 'embedding_matrix.csv'\n"
     ]
    }
   ],
   "source": [
    "np.savetxt('embedding_matrix.csv', embedding_matrix, delimiter=',')\n",
    "print(\"Embedding matrix saved as 'embedding_matrix.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Summary\n",
    "\n",
    "In this notebook, we:\n",
    "- Loaded and explored the training data.\n",
    "- Dropped unnecessary columns and cleaned the tweet text by removing noise (URLs, punctuation, stopwords, extra spaces).\n",
    "- Saved the preprocessed data as `train_prepro.csv`.\n",
    "- Tokenized the cleaned tweet text, padded the sequences, and created an embedding matrix using pretrained GloVe embeddings.\n",
    "- Saved the embedding matrix as `embedding_matrix.npy` for later use in model training.\n",
    "\n",
    "This workflow prepares the data for the next steps in building and training your offensive language detection models.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
